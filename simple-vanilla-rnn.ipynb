{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import data\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable logs\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "#\n",
    "# checkpoint\n",
    "ckpt_path = 'ckpt/vanilla1/'\n",
    "#\n",
    "###\n",
    "# get data\n",
    "X, Y, idx2ch, ch2idx = data.load_data('data/paulg/')\n",
    "#\n",
    "# params\n",
    "hsize = 256\n",
    "num_classes = len(idx2ch)\n",
    "seqlen = X.shape[1]\n",
    "state_size = hsize\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step operation\n",
    "def step(hprev, x):\n",
    "    # initializer\n",
    "    xav_init = tf.contrib.layers.xavier_initializer\n",
    "    # params\n",
    "    W = tf.get_variable('W', shape=[state_size, state_size], initializer=xav_init())\n",
    "    U = tf.get_variable('U', shape=[state_size, state_size], initializer=xav_init())\n",
    "    b = tf.get_variable('b', shape=[state_size], initializer=tf.constant_initializer(0.))\n",
    "    # current hidden state\n",
    "    h = tf.tanh(tf.matmul(hprev, W) + tf.matmul(x,U) + b)\n",
    "    return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse arguments\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Vanilla Recurrent Neural Network for Text Hallucination, built with tf.scan')\n",
    "    group = parser.add_mutually_exclusive_group(required=True)\n",
    "    group.add_argument('-g', '--generate', action='store_true',\n",
    "                        help='generate text')\n",
    "    group.add_argument('-t', '--train', action='store_true',\n",
    "                        help='train model')\n",
    "    parser.add_argument('-n', '--num_words', required=False, type=int,\n",
    "                        help='number of words to generate')\n",
    "    args = vars(parser.parse_args())\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    #\n",
    "    # parse arguments\n",
    "    args = parse_args()\n",
    "    #\n",
    "    # build graph\n",
    "    tf.reset_default_graph()\n",
    "    # inputs\n",
    "    xs_ = tf.placeholder(shape=[None, None], dtype=tf.int32)\n",
    "    ys_ = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "    #\n",
    "    # embeddings\n",
    "    embs = tf.get_variable('emb', [num_classes, state_size])\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embs, xs_)\n",
    "    #\n",
    "    # initial hidden state\n",
    "    init_state = tf.placeholder(shape=[None, state_size], dtype=tf.float32, name='initial_state')\n",
    "    #\n",
    "    # scan function\n",
    "    #   tf.scan(fn, elems, initializer)\n",
    "    states = tf.scan(step, \n",
    "            tf.transpose(rnn_inputs, [1,0,2]),\n",
    "            initializer=init_state) \n",
    "    ###\n",
    "    # set last state\n",
    "    last_state = states[-1]\n",
    "    states = tf.transpose(states, [1,0,2])\n",
    "    #\n",
    "    # predictions\n",
    "    V = tf.get_variable('V', shape=[state_size, num_classes], \n",
    "                        initializer=tf.contrib.layers.xavier_initializer())\n",
    "    bo = tf.get_variable('bo', shape=[num_classes], \n",
    "                         initializer=tf.constant_initializer(0.))\n",
    "    #\n",
    "    # flatten states to 2d matrix for matmult with V\n",
    "    states_reshaped = tf.reshape(states, [-1, state_size])\n",
    "    logits = tf.matmul(states_reshaped, V) + bo\n",
    "    predictions = tf.nn.softmax(logits)\n",
    "    #\n",
    "    # optimization\n",
    "    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, ys_)\n",
    "    loss = tf.reduce_mean(losses)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=0.1).minimize(loss)\n",
    "    # \n",
    "    # step to train\n",
    "    if args['train']:\n",
    "        # \n",
    "        # training\n",
    "        #  setup batches for training\n",
    "        epochs = 50\n",
    "        #\n",
    "        # set batch size\n",
    "        batch_size = BATCH_SIZE\n",
    "        train_set = utils.rand_batch_gen(X,Y,batch_size=batch_size)\n",
    "        # training session\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            train_loss = 0\n",
    "            try:\n",
    "                for i in range(epochs):\n",
    "                    for j in range(1000):\n",
    "                        xs, ys = train_set.__next__()\n",
    "                        _, train_loss_ = sess.run([train_op, loss], feed_dict = {\n",
    "                                xs_ : xs,\n",
    "                                ys_ : ys.reshape([batch_size*seqlen]),\n",
    "                                init_state : np.zeros([batch_size, state_size])\n",
    "                            })\n",
    "                        train_loss += train_loss_\n",
    "                    print('[{}] loss : {}'.format(i,train_loss/1000))\n",
    "                    train_loss = 0\n",
    "            except KeyboardInterrupt:\n",
    "                print('interrupted by user at ' + str(i))\n",
    "                #\n",
    "                # training ends here; \n",
    "                #  save checkpoint\n",
    "                saver = tf.train.Saver()\n",
    "                saver.save(sess, ckpt_path + 'vanilla1.ckpt', global_step=i)\n",
    "    elif args['generate']:\n",
    "        #\n",
    "        # generate text\n",
    "        random_init_word = random.choice(idx2ch)\n",
    "        current_word = ch2idx[random_init_word]\n",
    "        #\n",
    "        # start session\n",
    "        with tf.Session() as sess:\n",
    "            # init session\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            #\n",
    "            # restore session\n",
    "            ckpt = tf.train.get_checkpoint_state(ckpt_path)\n",
    "            saver = tf.train.Saver()\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            # generate operation\n",
    "            words = [current_word]\n",
    "            state = None\n",
    "            # set batch_size to 1\n",
    "            batch_size = 1\n",
    "            num_words = args['num_words'] if args['num_words'] else 111\n",
    "            # enter the loop\n",
    "            for i in range(num_words):\n",
    "                if state:\n",
    "                    feed_dict = { xs_ : np.array(current_word).reshape([1, 1]), \n",
    "                            init_state : state_ }\n",
    "                else:\n",
    "                    feed_dict = { xs_ : np.array(current_word).reshape([1,1])\n",
    "                            , init_state : np.zeros([batch_size, state_size]) }\n",
    "                #\n",
    "                # forward propagation\n",
    "                preds, state_ = sess.run([predictions, last_state], feed_dict=feed_dict)\n",
    "                # \n",
    "                # set flag to true\n",
    "                state = True\n",
    "                # \n",
    "                # set new word\n",
    "                current_word = np.random.choice(preds.shape[-1], 1, p=np.squeeze(preds))[0]\n",
    "                # add to list of words\n",
    "                words.append(current_word)\n",
    "        #########\n",
    "        # text generation complete\n",
    "        #\n",
    "        print('______Generated Text_______')\n",
    "        print(''.join([idx2ch[w] for w in words]))\n",
    "        print('___________________________')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
